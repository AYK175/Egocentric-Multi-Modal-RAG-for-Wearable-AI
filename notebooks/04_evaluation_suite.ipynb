{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9987749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os, json, re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ad5d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ecf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name = 'unsloth/gemma-3-12b-it',\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastVisionModel.for_inference(model)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874047c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3542a449b94b6d8afc9f7bfaf2ebf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_id = \"google/gemma-7b\"\n",
    "# model_id = \"google/medgemma-27b-text-it\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32, token=\"hf_token\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b733cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"crag-mm-2025/crag-mm-single-turn-public\")\n",
    "# Eval on public test set\n",
    "dataset = raw_dataset['public_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d881e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grading_prompt(prediction, ground_truth):\n",
    "    # Define the Grading Rubric\n",
    "    system_prompt = (\n",
    "        \"\"\"Based on my answer and the true answer score my answer based on:\n",
    "        ‚úÖ Perfect (fully correct) ‚Üí Score: 1.0\n",
    "        ‚ö† Acceptable (useful but with minor non-harmful errors) ‚Üí Score: 0.5\n",
    "        ‚ùì Missing (e.g., ‚ÄúI don‚Äôt know‚Äù, ‚ÄúI‚Äôm sorry I can‚Äôt find ‚Ä¶‚Äù) ‚Üí Score: 0.0\n",
    "        ‚ùå Incorrect (wrong or irrelevant answer) ‚Üí Score: -1.0\n",
    "\n",
    "        Return exactly one numeric score from the set {1.0, 0.5, 0.0, -1.0}.\n",
    "        No extra words. Only the score number.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Construct the Content\n",
    "    user_content = (\n",
    "        f\"Ground Truth:\\n{ground_truth}\\n\\n\"\n",
    "        f\"Model Prediction:\\n{prediction}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    # Apply Chat Template\n",
    "    text_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text=text_prompt, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f9bd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_dir = 'final_outputs/task1_answers_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d0f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_#': 0, 'question': 'is this a good car for transporting seven passengers at once?', 'prediction': 'The Subaru WRX STI is not a good car for transporting seven passengers at once. It is designed for performance and handling, not for transporting large groups of people. It has a seating capacity of 5 passengers. If you need to transport seven passengers, you may want to consider a larger vehicle with a higher seating capacity.', 'ground_truth': 'no, the subaru wrx is a compact car with a total passenger capacity of 5 people.'}\n"
     ]
    }
   ],
   "source": [
    "with open(f'{task1_dir}/task1_results_0-9.json', 'r') as f:\n",
    "    test = json.load(f)\n",
    "    print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 evaluation\n",
    "task1_files = os.listdir(task1_dir)\n",
    "task1_data = {\n",
    "    \"predictions\": [],\n",
    "    \"ground_truths\": [],\n",
    "}\n",
    "scores = []\n",
    "\n",
    "for filename in task1_files:\n",
    "    with open(os.path.join(task1_dir, filename), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for sample in data:\n",
    "            task1_data[\"predictions\"].append(sample['prediction'])\n",
    "            task1_data[\"ground_truths\"].append(sample['ground_truth'])\n",
    "\n",
    "for i, query in enumerate(tqdm(task1_data['predictions'])):\n",
    "    prediction = task1_data['predictions'][i]\n",
    "    ground_truth = task1_data['ground_truths'][i]\n",
    "    inputs = build_grading_prompt(prediction, ground_truth)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.01,\n",
    "        )\n",
    "\n",
    "    generated_ids = output[0][inputs.input_ids.shape[1]:]\n",
    "    response_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract Score using Regex\n",
    "    # Looks for 1.0, 0.5, 0.0, or -1.0\n",
    "    match = re.search(r\"(-?1\\.0|0\\.5|0\\.0)\", response_text)\n",
    "    \n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "    else:\n",
    "        print(f\"‚ö† Warning: Could not parse score from: '{response_text}'. Defaulting to 0.0\")\n",
    "        score = 0.0 # Default fallback\n",
    "        \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7054ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rating_prompt(sys_prompt: str, model_resp: str, gold: str) -> str:\n",
    "    return (\n",
    "        f\"{sys_prompt}\\n\\n\"\n",
    "        f\"Model answer:\\n{model_resp}\\n\\n\"\n",
    "        f\"True answer:\\n{gold}\\n\\n\"\n",
    "        f\"Score:\"\n",
    "    )\n",
    "\n",
    "system_prompt = (\n",
    "        \"\"\"Based on my answer and the true answer score my answer based on:\n",
    "        ‚úÖ Perfect (fully correct) ‚Üí Score: 1.0\n",
    "        ‚ö† Acceptable (useful but with minor non-harmful errors) ‚Üí Score: 0.5\n",
    "        ‚ùì Missing (e.g., ‚ÄúI don‚Äôt know‚Äù, ‚ÄúI‚Äôm sorry I can‚Äôt find ‚Ä¶‚Äù) ‚Üí Score: 0.0\n",
    "        ‚ùå Incorrect (wrong or irrelevant answer) ‚Üí Score: -1.0\n",
    "\n",
    "        Return exactly one numeric score from the set {1.0, 0.5, 0.0, -1.0}.\n",
    "        No extra words. Only the score number.\"\"\"\n",
    "    )\n",
    "# Task 1 evaluation\n",
    "task1_files = os.listdir(task1_dir)\n",
    "task1_data = {\n",
    "    \"predictions\": [],\n",
    "    \"ground_truths\": [],\n",
    "}\n",
    "scores = []\n",
    "\n",
    "for filename in task1_files:\n",
    "    with open(os.path.join(task1_dir, filename), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for sample in data:\n",
    "            task1_data[\"predictions\"].append(sample['prediction'])\n",
    "            task1_data[\"ground_truths\"].append(sample['ground_truth'])\n",
    "\n",
    "# --- BATCH PROCESSING SETUP ---\n",
    "# Set padding side to left for generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "batch_size = 8  # Adjust this based on your GPU memory (try 4, 8, or 16)\n",
    "predictions = task1_data['predictions']\n",
    "ground_truths = task1_data['ground_truths']\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(predictions), batch_size), desc=\"Evaluating batches\"):\n",
    "    batch_preds = predictions[i : i + batch_size]\n",
    "    batch_gts = ground_truths[i : i + batch_size]\n",
    "    \n",
    "    # 1. Build text prompts for the entire batch\n",
    "    text_prompts = [\n",
    "        build_rating_prompt(system_prompt, p, g) \n",
    "        for p, g in zip(batch_preds, batch_gts)\n",
    "    ]\n",
    "\n",
    "    # 2. Tokenize the batch (this converts strings to model inputs)\n",
    "    inputs = tokenizer(\n",
    "        text_prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 3. Generate scores\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10, # We only need a short number, 300 is unnecessary\n",
    "            temperature=0.01,\n",
    "        )\n",
    "\n",
    "    # 4. Decode batch results\n",
    "    # Slice [:, inputs.shape[1]:] to get only the newly generated tokens\n",
    "    generated_ids = output[:, inputs.input_ids.shape[1]:]\n",
    "    decoded_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 5. Parse scores\n",
    "    for response_text in decoded_responses:\n",
    "        match = re.search(r\"(-?1\\.0|0\\.5|0\\.0)\", response_text)\n",
    "        \n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "        else:\n",
    "            print(f\"‚ö† Warning: Could not parse score from: '{response_text}'. Defaulting to 0.0\")\n",
    "            score = 0.0 \n",
    "            \n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1107784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation Complete.\n",
      "Average Accuracy Score: 0.3742\n"
     ]
    }
   ],
   "source": [
    "# Calculate Final Stats\n",
    "score = np.sum(scores)/len(scores)\n",
    "print(f\"\\n‚úÖ Evaluation Complete.\")\n",
    "print(f\"Average Accuracy Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea9ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_file = 'Task2_final_answers.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be448e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_answers = []\n",
    "task2_scores = []\n",
    "with open(task2_file, 'r') as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        task2_answers.append(sample['answer'])\n",
    "\n",
    "for i, answer in enumerate(tqdm(task2_answers)):\n",
    "    ground_truth = task1_data['ground_truths'][i]\n",
    "    inputs = build_grading_prompt(answer, ground_truth)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.01,\n",
    "        )\n",
    "\n",
    "    generated_ids = output[0][inputs.input_ids.shape[1]:]\n",
    "    response_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract Score using Regex\n",
    "    # Looks for 1.0, 0.5, 0.0, or -1.0\n",
    "    match = re.search(r\"(-?1\\.0|0\\.5|0\\.0)\", response_text)\n",
    "    \n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "    else:\n",
    "        print(f\"‚ö† Warning: Could not parse score from: '{response_text}'. Defaulting to 0.0\")\n",
    "        score = 0.0 # Default fallback\n",
    "        \n",
    "    task2_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4dceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Final Stats\n",
    "score = np.sum(task2_scores)/len(task2_scores)\n",
    "print(f\"\\n‚úÖ Evaluation Complete.\")\n",
    "print(f\"Average Accuracy Score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
