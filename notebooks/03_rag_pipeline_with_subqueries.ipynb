{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import List, Any\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "load_dotenv()\n",
    "\n",
    "if 'HF_TOKEN' not in os.environ:\n",
    "    raise ValueError(\"HF_TOKEN environment variable is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d75a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"crag-mm-2025/crag-mm-single-turn-public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf64a1",
   "metadata": {},
   "source": [
    "#### **Using Llama 8B to generate sub-queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4009cc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fa3841f6ec4a9896a50386aabf42e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", dtype=\"auto\", device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(\"Model device:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ead05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 implementation to generate sub question and query it to web search \n",
    "class Task2SubQuestionGenerator:\n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        # model for subquestion generation\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        from cragmm_search.search import UnifiedSearchPipeline\n",
    "        # initiate both image and web search API\n",
    "        ## validation\n",
    "        self.search_pipeline = UnifiedSearchPipeline(\n",
    "            image_model_name=\"openai/clip-vit-large-patch14-336\",\n",
    "            image_hf_dataset_id=\"crag-mm-2025/image-search-index-validation\",\n",
    "            text_model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "            web_hf_dataset_id=\"crag-mm-2025/web-search-index-validation\",\n",
    "        )\n",
    "    def remove_none_recursively(self, data) -> dict:\n",
    "        \"\"\"\n",
    "        Recursively removes keys from a dictionary (and nested dictionaries/lists)\n",
    "        where the value is None, an empty dictionary, or an empty list.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            # First, recursively clean the values\n",
    "            cleaned_data = {\n",
    "                k: self.remove_none_recursively(v) \n",
    "                for k, v in data.items()\n",
    "            }\n",
    "            # Then, remove keys where the cleaned value is None or empty\n",
    "            return {\n",
    "                k: v for k, v in cleaned_data.items() \n",
    "                if v is not None and v not in [{}, [], '<>', \"<>\"]\n",
    "            }\n",
    "        elif isinstance(data, list):\n",
    "            cleaned_list = [self.remove_none_recursively(item) for item in data]\n",
    "            # Remove None and empty items from the list\n",
    "            return [\n",
    "                item for item in cleaned_list \n",
    "                if item is not None and item != {} and item != []\n",
    "            ]\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    def generate_sub_question(self, query, image) -> List[str]:\n",
    "        SYSTEM_PROMPT=\"\"\"You are an expert query decomposition engine for a Retrieval-Augmented Generation (RAG) system. Your task is to analyze the user's complex input and break it down into distinct, simple sub-queries that are necessary to gather facts to answer the original question.\n",
    "\n",
    "        Logic for Decomposition:\n",
    "        1. Analyze the user's request to identify the underlying logical steps or variables needed.\n",
    "        2. If the user asks a comparison question (e.g., \"Is X better than Y?\"), generate separate queries for the attributes of X and Y.\n",
    "        3. If the user asks a conditional question (e.g., \"Can a Toyota drive from A to B with 5 gallons?\"), generate separate queries for the distance between A and B, and the fuel efficiency/tank capacity of the Toyota.\n",
    "        4. Ensure every sub-query focuses on retrieving specific, factual information.\n",
    "\n",
    "        Constraints:\n",
    "        - Output only the list of sub-queries, one per line.\n",
    "        - Do not include any conversational phrases, explanations, or numbering.\n",
    "        - Focus on keywords and specific entities.\n",
    "        - Do not include quotation marks.\n",
    "\n",
    "        User Input: [Insert User Input Here]\n",
    "        \"\"\"\n",
    "        # The function query the entities by image first. combine Phong's version\n",
    "        # The output should be same as List[dict[str, Any]] with score and entities\n",
    "        image_response = self.search_pipeline(image, k=1)\n",
    "        extracted_entities = self.remove_none_recursively(image_response)\n",
    "        text_query = f\"query: {query}. Entity:'entity_name': '{extracted_entities}'. Generate sub-question that will help answer the query.\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": text_query},\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "                temperature=1.0,\n",
    "                pad_token_id=128001,\n",
    "            )\n",
    "        model_response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        model_response = model_response.split(\"assistant\")[-1].strip()\n",
    "        filtered_questions = list(dict.fromkeys(model_response.split('\\n')))\n",
    "        return filtered_questions[0]\n",
    "    \n",
    "    def sub_question_web_search(self, image, query: str, k=1) -> str:\n",
    "        sub_question = self.generate_sub_question(query, image)\n",
    "        response = self.search_pipeline(sub_question, k=k)\n",
    "        return self.clean_up_search_results(response)        \n",
    "\n",
    "    def clean_up_search_results(self, results: List[List[dict[str, Any]]]) -> str:\n",
    "        retrieval_documents = []\n",
    "        for index, item in enumerate(results):\n",
    "            retrieval_documents.append(f\"Document {index+1}: {item[0]['page_snippet']}\")\n",
    "        return \"\\n\".join(retrieval_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcfbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading web search data from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408d48fd4d734a53b058a34bc6edcaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb481c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48370>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.3s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb483a0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded collection with 904899 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48640>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48a00>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 1.2s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48a30>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48e20>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb49090>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 3.5s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb490c0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb494b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb49630>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|ERROR]Giving up send_request(...) after 4 tries (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76608ff73cd0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb49090>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb488e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.6s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48640>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48670>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48130>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.5s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb48370>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb481f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb499f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 3.1s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb49a20>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb49cc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb4a080>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|ERROR]Giving up send_request(...) after 4 tries (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76606cb4a0b0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n"
     ]
    }
   ],
   "source": [
    "question_generator = Task2SubQuestionGenerator(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753464b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Task2_web_search_results.jsonl\"\n",
    "\n",
    "folder = \"final_outputs/crop\"\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    raise FileNotFoundError(\"You have to run Task1 first to get the crop image.\")\n",
    "\n",
    "png_files = sorted(\n",
    "    [f for f in os.listdir(folder) if f.lower().endswith(\".png\")],\n",
    "    key=lambda x: int(os.path.splitext(x)[0])\n",
    ")\n",
    "\n",
    "processed_queries = set()\n",
    "\n",
    "if os.path.exists(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                processed_queries.add(data['query'])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "print(f\"Found {len(processed_queries)} items already processed.\")\n",
    "\n",
    "\n",
    "with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    for index in tqdm(range(len(png_files)), desc=\"Processing items\"):\n",
    "        image_path = os.path.join(folder, png_files[index])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        item = dataset['validation'][index]\n",
    "            \n",
    "        current_query = item['turns']['query'][0]\n",
    "        \n",
    "        # SKIP if we have done this one already\n",
    "        if current_query in processed_queries:\n",
    "            continue\n",
    "        \n",
    "        # Run the sub question generation and web search\n",
    "        response = question_generator.sub_question_web_search(\n",
    "            image, \n",
    "            current_query, \n",
    "            k=1\n",
    "        )\n",
    "        \n",
    "        entry = {\n",
    "            \"query\": current_query,\n",
    "            \"response\": response\n",
    "        }\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "        processed_queries.add(current_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "path = \"Task2_web_search_results_k_1.jsonl\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(query, retrieval_documents, image, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Build RAG prompt with image and return tokenized inputs ready for generation.\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        retrieval_documents: Retrieved document snippets\n",
    "        image: PIL Image or image data\n",
    "        tokenizer: Model tokenizer\n",
    "        model: Model instance (for device placement)\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs ready for model.generate()\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert assistant. Answer the user question ONLY based on \"\n",
    "                \"the provided retrieved documents. If the documents do not contain \"\n",
    "                \"enough information, say 'I do not have enough information to answer.' \"\n",
    "                \"Do NOT hallucinate.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"<|image|>Retrieved Documents:\\n{retrieval_documents}\\n\\n\"\n",
    "                f\"User Question:\\n{query}\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Convert to final prompt using Llama chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize with image and move to device\n",
    "    inputs = tokenizer(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model, llama_tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name = 'unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit',\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastVisionModel.for_inference(llama_model)\n",
    "llama_tokenizer = get_chat_template(\n",
    "    llama_tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d368b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"Task2_final_answers.jsonl\"\n",
    "\n",
    "# Check what's already processed\n",
    "processed_indices = set()\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                df = json.loads(line)\n",
    "                processed_indices.add(df['index'])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "print(f\"Found {len(processed_indices)} items already processed.\")\n",
    "\n",
    "# Process and save answers\n",
    "with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    for idx in tqdm(range(len(processed_indices)), desc=\"Generating ianswer\"):\n",
    "        item = dataset['validation'][idx]\n",
    "        # Skip already processed items\n",
    "        if idx in processed_indices:\n",
    "            continue\n",
    "            \n",
    "        if item['image'] is None:\n",
    "            continue\n",
    "            \n",
    "        query = item['turns']['query'][0]\n",
    "        image = item['image']\n",
    "        prompt = data[idx]['response']\n",
    "\n",
    "        inputs = build_rag_prompt(query, prompt, image, llama_tokenizer, llama_model)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = llama_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                pad_token_id=llama_tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the assistant's answer (after the last \"assistant\" marker)\n",
    "        final_answer = full_response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        entry = {\n",
    "            \"index\": idx,\n",
    "            \"query\": query,\n",
    "            \"answer\": final_answer\n",
    "        }\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "        f.flush()  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
