{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a4cb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdb8bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch, json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from unsloth import FastVisionModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, Florence2ForConditionalGeneration\n",
    "from cragmm_search.search import UnifiedSearchPipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1044e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading web search data from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c661aa7667554677a182786136bb4c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded collection with 904899 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9f5e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9f790>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.1s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9fa90>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9f8b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9fe20>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.5s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9fe50>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72950280>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a729504f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 2.3s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72950520>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a729507c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72950b80>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|ERROR]Giving up send_request(...) after 4 tries (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72950bb0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72951030>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72951630>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.6s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72951690>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72951900>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9fe50>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.8s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9fc40>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72c9fac0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72951060>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.1s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72950e20>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72951600>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a729508b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|ERROR]Giving up send_request(...) after 4 tries (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72950af0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image index from huggingface crag-mm-2025/image-search-index-validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88147cada036403986e0a7d4a86084c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9857460>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e98578b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.8s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9857820>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9857e80>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9856fe0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.5s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9857910>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9857790>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9856a10>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 3.7s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9855390>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "search_pipeline = UnifiedSearchPipeline(\n",
    "    image_model_name=\"openai/clip-vit-large-patch14-336\",\n",
    "    image_hf_dataset_id=\"crag-mm-2025/image-search-index-validation\",\n",
    "    text_model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    web_hf_dataset_id=\"crag-mm-2025/web-search-index-validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b6d76",
   "metadata": {},
   "source": [
    "# Configs\n",
    "Steps are defined to load/unload models in different steps. Kill the notebook process and restart the kernel process to release all memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6f2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\"entity_query\", \"crop\", \"task1\", \"web_query\", \"task2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8b9bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 1938\n",
      "Filtered size: 1423\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"crag-mm-2025/crag-mm-single-turn-public\")\n",
    "# Use lazy loading to avoid running out of memory\n",
    "dataset = raw_dataset['validation'].filter(lambda example: example['image'] is not None)\n",
    "\n",
    "print(f\"Original size: {len(raw_dataset['validation'])}\")\n",
    "print(f\"Filtered size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f660eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from: task1\n"
     ]
    }
   ],
   "source": [
    "total_items = len(dataset)\n",
    "output_dir = \"final_outputs\"\n",
    "use_cropped_images = True\n",
    "task1_out_dir = 'task1_answers_finetune'\n",
    "crop_dir = 'crop'\n",
    "entity_queries_file = \"entity_queries.txt\"\n",
    "\n",
    "# create output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, crop_dir), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, task1_out_dir), exist_ok=True)\n",
    "\n",
    "starts_from_index = 0\n",
    "\n",
    "# Fast track to skip completed steps\n",
    "if os.path.exists(os.path.join(output_dir, entity_queries_file)):\n",
    "    starts_from_index += 1\n",
    "if os.path.exists(os.path.join(output_dir, crop_dir, f\"{total_items - 1}.png\")):\n",
    "    starts_from_index += 1\n",
    "if os.path.exists(os.path.join(output_dir, task1_out_dir, \"task1_done\")):\n",
    "    starts_from_index += 1\n",
    "\n",
    "print(\"Starting from:\", steps[starts_from_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17312330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_from_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af744c0f",
   "metadata": {},
   "source": [
    "# Generate image query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d365e762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unsloth', 'zephyr', 'chatml', 'mistral', 'llama', 'vicuna', 'vicuna_old', 'vicuna old', 'alpaca', 'gemma', 'gemma_chatml', 'gemma2', 'gemma2_chatml', 'llama-3', 'llama3', 'phi-3', 'phi-35', 'phi-3.5', 'llama-3.1', 'llama-31', 'llama-3.2', 'llama-3.3', 'llama-32', 'llama-33', 'qwen-2.5', 'qwen-25', 'qwen25', 'qwen2.5', 'phi-4', 'gemma-3', 'gemma3', 'qwen-3', 'qwen3', 'gemma-3n', 'gemma3n', 'gpt-oss', 'gptoss', 'qwen3-instruct', 'qwen3-thinking', 'lfm-2', 'starling', 'yi-chat']\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import CHAT_TEMPLATES\n",
    "print(list(CHAT_TEMPLATES.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abbf386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726a72cc02e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e9856890>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|ERROR]Giving up send_request(...) after 4 tries (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x72684e1f71f0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.6: Fast Mllama patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.569 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dc9352c44c4c38a3c90778f84df5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llm/projects/CRAG/.venv/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use llama 3.2 for all steps except cropping\n",
    "if steps[starts_from_index] != \"crop\":\n",
    "    # Load model with unsloth\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name = 'unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit',\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, '/home/llm/projects/CRAG/llama3.2-11B-q4-bf16')\n",
    "    model = model.merge_and_unload()  # Optional: merge adapter into base\n",
    "\n",
    "    FastVisionModel.for_inference(model)\n",
    "    llama_tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3.2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feac6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_query(image: Image, user_prompt: str) -> str:\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are an expert visual assistant. \"\n",
    "        \"Given the image and the user query, identify the main subject \"\n",
    "        \"in the image that the query refers to. \"\n",
    "        \"Output ONLY the simple object name (e.g., 'black car', 'potted plant'). \"\n",
    "        \"The subject always appears near the center of the image. \"\n",
    "        \"DO NOT answer the query. Do not output a sentence. No punctuation. \"\n",
    "        \"I REPEAT: Output ONLY the object's name, and NOTHING ELSE. \"\n",
    "        \"Answer in less than 5 words.\"\n",
    "    )\n",
    "\n",
    "    content_str = f\"<|image|>{user_prompt}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": content_str}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False \n",
    "    )\n",
    "\n",
    "    # In Unsloth, the tokenizer acts as the processor\n",
    "    inputs = tokenizer(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=5) # Short answer only\n",
    "    generated_ids = output[0][inputs.input_ids.shape[1]:]\n",
    "    decoded_resp = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return decoded_resp.split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a605edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps[starts_from_index] == \"entity_query\":\n",
    "    results = []\n",
    "\n",
    "    for sample in tqdm(dataset):\n",
    "        image = sample['image']\n",
    "        question = sample['turns']['query'][0]\n",
    "\n",
    "        try:\n",
    "            prediction = generate_entity_query(image=image, user_prompt=question)\n",
    "            results.append(prediction)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample: {e}\")\n",
    "\n",
    "        if results:\n",
    "            with open(os.path.join(output_dir, entity_queries_file), 'w') as f:\n",
    "                f.write('\\n'.join(results))\n",
    "    \n",
    "    starts_from_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd66d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_from_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5f98b",
   "metadata": {},
   "source": [
    "# Cropping\n",
    "Define cropping function using Florence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccac727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlorencePipeline:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Running on device: {self.device}\")\n",
    "\n",
    "        # 1. Initialize Florence-2 (The \"All-in-One\" Vision Model)\n",
    "        # We use 'base' for speed, or 'large' for slightly better accuracy.\n",
    "        # This REPLACES both Llama 3.2 AND Grounding DINO for the cropping task.\n",
    "        self.model_id = \"florence-community/Florence-2-base\"\n",
    "        # print(f\"Loading {self.model_id}...\")\n",
    "\n",
    "        # trust_remote_code=True is required for Florence-2\n",
    "        self.model = Florence2ForConditionalGeneration.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_id, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "    def ground_and_crop(self, image, query):\n",
    "        \"\"\"\n",
    "        Uses Florence-2 to find the object described by 'query' and crop the image.\n",
    "        Accepts PIL Image directly.\n",
    "        \"\"\"\n",
    "        # print(f\"-> Grounding '{query}'...\")\n",
    "        w, h = image.size\n",
    "\n",
    "        # Florence-2 Task: <CAPTION_TO_PHRASE_GROUNDING>\n",
    "        # We append the query to the task token\n",
    "        task_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "        text_input = task_prompt + query\n",
    "\n",
    "        # Prepare inputs (Direct PIL Image support)\n",
    "        inputs = self.processor(\n",
    "            text=text_input,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device, torch.float16 if self.device == \"cuda\" else torch.float32)\n",
    "\n",
    "        # Generate predictions\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "        )\n",
    "\n",
    "        # Decode output text\n",
    "        generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "        # Post-process to get boxes (Florence returns specific format)\n",
    "        parsed_answer = self.processor.post_process_generation(\n",
    "            generated_text,\n",
    "            task=task_prompt,\n",
    "            image_size=(image.width, image.height)\n",
    "        )\n",
    "\n",
    "        # parsed_answer is usually: {'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2]], 'labels': ['car']}}\n",
    "        grounding_result = parsed_answer.get(task_prompt, {})\n",
    "        bboxes = grounding_result.get('bboxes', [])\n",
    "\n",
    "        if not bboxes:\n",
    "            # print(\"-> No object found. Using full image.\")\n",
    "            return False, (0, 0, w, h), image\n",
    "\n",
    "        # Take the first box (highest confidence/relevance usually first)\n",
    "        box = bboxes[0]\n",
    "\n",
    "        # Crop (Florence returns [x1, y1, x2, y2] absolute coordinates)\n",
    "        # Ensure coordinates are within bounds\n",
    "        x1 = max(0, box[0])\n",
    "        y1 = max(0, box[1])\n",
    "        x2 = min(image.width, box[2])\n",
    "        y2 = min(image.height, box[3])\n",
    "\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            # print(\"-> Invalid box dimensions.\")\n",
    "            return False, (0, 0, w, h), image\n",
    "\n",
    "        tolerance = 10 # pixels\n",
    "        is_whole_image = (x1 < tolerance and y1 < tolerance and x2 > (w - tolerance) and y2 > (h - tolerance))\n",
    "\n",
    "        if is_whole_image:\n",
    "            # print(f\"-> Found object, but it spans the whole image {x1, y1, x2, y2}.\")\n",
    "            # We return False for 'crop success', but return the coordinates found\n",
    "            return False, (x1, y1, x2, y2), image\n",
    "\n",
    "        cropped_img = image.crop((x1, y1, x2, y2))\n",
    "        return True, (x1, y1, x2, y2), cropped_img\n",
    "\n",
    "    def run(self, image, user_query):\n",
    "        # print(f\"--- Processing: {user_query} ---\")\n",
    "\n",
    "        # Step 1: Ground & Crop (Florence-2)\n",
    "        # Note: We pass the RAW user query directly. Florence is smart enough to extract\n",
    "        # entities from questions like \"Where is the black car?\" -> it looks for \"black car\".\n",
    "        cropped_image = self.ground_and_crop(image, user_query)\n",
    "\n",
    "        # Step 2: Search (CRAG)\n",
    "        search_results = search_pipeline.search(\n",
    "            query_text=user_query,\n",
    "            query_image=cropped_image,\n",
    "            k=5\n",
    "        )\n",
    "\n",
    "        if search_results:\n",
    "            print(search_results[0])\n",
    "\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a23d9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps[starts_from_index] == \"crop\":\n",
    "    crop_pipeline = FlorencePipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e127b",
   "metadata": {},
   "source": [
    "### Define prompts for cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e33a6",
   "metadata": {},
   "source": [
    "This is only a simple prompt mapping based on domain of the query. Use this if image-specific prompts generated by llama3.2 are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0de341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the fact that most photos have the subject near the center, for domains with\n",
    "# no specific category we use a generic prompt.\n",
    "GENERIC_PROMPT = \"objects, animals or people near the center of the screen\"\n",
    "florence_prompts = {\n",
    "    1: \"book\",\n",
    "    3: \"structure\",\n",
    "    4: \"item\",\n",
    "    5: \"plant\",\n",
    "    6: \"food\",\n",
    "    8: \"building\",\n",
    "    9: \"vehicle\",\n",
    "    'other': GENERIC_PROMPT\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f92b1c",
   "metadata": {},
   "source": [
    "### Generate cropped samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24f46e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps[starts_from_index] == \"crop\":\n",
    "    stats = []\n",
    "    with open(os.path.join(output_dir, entity_queries_file), 'r') as f:\n",
    "        entity_prompts = f.read().splitlines()\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        if not os.path.exists(os.path.join(output_dir, crop_dir, f\"{i}.png\")):\n",
    "            domain = sample['turns']['domain'][0]\n",
    "            \n",
    "            success, bounds, cropped_image = crop_pipeline.ground_and_crop(sample['image'], entity_prompts[i])\n",
    "            if not success:\n",
    "                # Retry with generic prompt\n",
    "                success, bounds, cropped_image = crop_pipeline.ground_and_crop(sample['image'], florence_prompts.get(domain, 'other'))\n",
    "\n",
    "            cropped_image.save(os.path.join(output_dir, crop_dir, f\"{i}.png\"))\n",
    "            stats.append(f\"{i}: success [{success}] prompt [{domain}] bounding box [{bounds}]\")\n",
    "            with open(os.path.join(output_dir, crop_dir, \"stats.txt\"), \"w\") as f:\n",
    "                f.write(\"\\n\".join(stats))\n",
    "\n",
    "    starts_from_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c5bdd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_from_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952831",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Evaluation using llama 3.2 11B model and cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92bab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 generate functions\n",
    "def clean_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        cleaned = {}\n",
    "        for k, v in obj.items():\n",
    "            v = clean_dict(v)  # recursive clean\n",
    "            # keep only non-empty and non-None values\n",
    "            if v not in (None, {}, []):\n",
    "                cleaned[k] = v\n",
    "        return cleaned if cleaned else None\n",
    "    elif isinstance(obj, list):\n",
    "        cleaned_list = [clean_dict(v) for v in obj if v is not None]\n",
    "        cleaned_list = [v for v in cleaned_list if v not in (None, {}, [])]\n",
    "        return cleaned_list if cleaned_list else None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def filter_and_clean(data, score_threshold=0.6):\n",
    "    \"\"\"Filter by score first, then clean the structure.\"\"\"\n",
    "    filtered = [d for d in data if d.get(\"score\", 0) >= score_threshold]\n",
    "    return [str(clean_dict(d)['entities']) for d in filtered]\n",
    "\n",
    "def generate_answer_v2(image: Image, user_prompt: str) -> str:\n",
    "    raw_results = search_pipeline(image, k=3)\n",
    "    cleaned_results_list = filter_and_clean(raw_results)\n",
    "\n",
    "    if cleaned_results_list:\n",
    "        context_str = '\\n'.join(cleaned_results_list)\n",
    "    else:\n",
    "        context_str = \"No specific external information found.\"\n",
    "\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a helpful assistant that truthfully answers user questions about the \"\n",
    "        \"provided image using the provided reference information. \"\n",
    "        \"Keep your response concise. If the answer is not in the image or references, \"\n",
    "        \"respond with 'I don't know'.\"\n",
    "    )\n",
    "    text_payload = f\"Reference Information:\\n{context_str}\\nUser Question: {user_prompt}\"\n",
    "    content_str = f\"<|image|>{text_payload}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": content_str} # Passing string prevents TypeError\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    # In Unsloth, the tokenizer acts as the processor\n",
    "    inputs = tokenizer(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=70) # Max allowed by competition\n",
    "    decoded_resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Safe extraction of the assistant's answer\n",
    "    generated_ids = output[0][inputs.input_ids.shape[1]:]\n",
    "    decoded_resp = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return decoded_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1cddb",
   "metadata": {},
   "source": [
    "### Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544506c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_from_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0ea5384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1423 [00:00<?, ?it/s][urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726704460850>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726704460be0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 0.5s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726704460c40>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7267042169e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7267042927d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 1.9s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x72670423fbe0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7267042f25c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726704433070>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|INFO]Backing off send_request(...) for 3.9s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726704430eb0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=1, connect=1, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x726704433c40>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[urllib3.connectionpool|WARNING]Retrying (Retry(total=0, connect=0, read=2, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e89edab0>: Failed to establish a new connection: [Errno 111] Connection refused')': /batch/\n",
      "[backoff|ERROR]Giving up send_request(...) after 4 tries (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7266e89ef160>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "  1%|          | 10/1423 [01:11<2:49:18,  7.19s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving task1_results_0-9.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 20/1423 [02:01<2:05:57,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving task1_results_10-19.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 30/1423 [02:41<1:33:24,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving task1_results_20-29.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 40/1423 [04:05<2:10:13,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving task1_results_30-39.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 50/1423 [04:45<1:29:11,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving task1_results_40-49.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 60/1423 [05:14<1:18:31,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving task1_results_50-59.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–         | 69/1423 [06:00<1:57:55,  5.23s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "image file is truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/CRAG/.venv/lib/python3.10/site-packages/PIL/ImageFile.py:370\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/CRAG/.venv/lib/python3.10/site-packages/PIL/PngImagePlugin.py:994\u001b[0m, in \u001b[0;36mPngImageFile.load_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# CRC\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m cid, pos, length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDAT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDAT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfdAT\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/projects/CRAG/.venv/lib/python3.10/site-packages/PIL/PngImagePlugin.py:176\u001b[0m, in \u001b[0;36mChunkStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 176\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[43mi32\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cid(cid):\n",
      "File \u001b[0;32m~/projects/CRAG/.venv/lib/python3.10/site-packages/PIL/_binary.py:95\u001b[0m, in \u001b[0;36mi32be\u001b[0;34m(c, o)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mi32be\u001b[39m(c: \u001b[38;5;28mbytes\u001b[39m, o: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munpack_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>I\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31merror\u001b[0m: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Switch for cropped vs original image with fall back if cropping fails\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cropped_images \u001b[38;5;129;01mand\u001b[39;00m crop_status[i]:\n\u001b[0;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     image \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/CRAG/.venv/lib/python3.10/site-packages/PIL/Image.py:986\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    984\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/CRAG/.venv/lib/python3.10/site-packages/PIL/ImageFile.py:377\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s:  \u001b[38;5;66;03m# truncated jpeg\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "\u001b[0;31mOSError\u001b[0m: image file is truncated"
     ]
    }
   ],
   "source": [
    "task1_signal = os.path.exists(os.path.join(output_dir, 'task1_done'))\n",
    "if steps[starts_from_index] == \"task1\" and not task1_signal:\n",
    "    # Output file for results\n",
    "    results = []\n",
    "\n",
    "    batch_size = 10  # Renamed from 'range' (don't shadow built-in types)\n",
    "    continue_from = -1 # Set to 50, 60, etc. to resume\n",
    "    current_batch_start = continue_from + 1\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_dir, crop_dir, \"stats.txt\"), 'r') as f:\n",
    "        crop_status = f.readlines()\n",
    "        crop_status = [True if \"[True]\" in line else False for line in crop_status]\n",
    "\n",
    "    print(\"Starting validation loop...\")\n",
    "    # Iterate through a subset for testing\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        # Continue from interruption\n",
    "        if i <= continue_from:\n",
    "            continue\n",
    "        \n",
    "        # Switch for cropped vs original image with fall back if cropping fails\n",
    "        if use_cropped_images and crop_status[i]:\n",
    "            image = Image.open(os.path.join(output_dir, crop_dir, f\"{i}.png\")).convert(\"RGB\")\n",
    "        else:\n",
    "            image = sample.get('image', None)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        question = sample['turns']['query'][0]\n",
    "        ground_truth = sample['answers']['ans_full'][0]\n",
    "\n",
    "        try:\n",
    "            prediction = generate_answer_v2(image, question)\n",
    "            results.append({\n",
    "                \"question_#\": i,\n",
    "                \"question\": question,\n",
    "                \"prediction\": prediction,\n",
    "                \"ground_truth\": ground_truth\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample: {e}\")\n",
    "\n",
    "        # Batch saving results\n",
    "        if (i + 1) % batch_size == 0 or i == total_items - 1:\n",
    "            if len(results) > 0:\n",
    "                filename = f\"task1_results_{current_batch_start}-{i}.json\"\n",
    "                print(f\"Saving {filename}...\")\n",
    "\n",
    "                out_file = os.path.join(output_dir, task1_out_dir, filename)\n",
    "                with open(out_file, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                results = []\n",
    "            current_batch_start = i + 1\n",
    "\n",
    "            if i == total_items - 1:\n",
    "                print(\"Completed all items.\")\n",
    "                with open(task1_signal, 'w') as f:\n",
    "                    f.write('done')\n",
    "\n",
    "starts_from_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_from_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
